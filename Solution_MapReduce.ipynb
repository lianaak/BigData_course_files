{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "666a53e6-7102-4fcb-9539-7d722b901413",
   "metadata": {},
   "source": [
    "# *MapReduce* <br> Stromverbrauch am Tag pro Kunde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5860ef-6a2a-4626-a9a5-35ee587769a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mapper.py\n",
    "#!/bin/python3\n",
    "import sys\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # split the line into words\n",
    "    words = line.split(\",\")\n",
    "\n",
    "    # key: constructing the customerID_date key by concatenating the\n",
    "    # value of the last element (column) which is the ID and the \n",
    "    # fourth element which is the date\n",
    "    # value: the consumption of a time interval in a day, which is \n",
    "    # seventh element\n",
    "    customerID_date= words[-1] + \"_\" + words[3]\n",
    "    consumptionPerDay = words[6]\n",
    "\n",
    "    # printing the key and the value to STDOUT\n",
    "    # this line will be sorted before passed to reducer\n",
    "    print(\"{}\\t{}\".format(customerID_date, consumptionPerDay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c488c422-3877-4a99-9b9a-aa6d5571f622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/bin/python3\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "currentCustomer = None\n",
    "currentConsumption = 0\n",
    "consumption = None\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    customerID_date, consumptionPerDay = line.split(\"\\t\", 1)\n",
    "\n",
    "    # convert consumptionPerDay (currently a string) to int\n",
    "    try:\n",
    "        consumptionPerDay = int(consumptionPerDay)\n",
    "    except ValueError:\n",
    "        # consumptionPerDay was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: customerID_date) before it is passed to the reducer\n",
    "    if currentCustomer == customerID_date:\n",
    "        currentConsumption += consumptionPerDay\n",
    "    else:\n",
    "        if currentConsumption:\n",
    "            # we write the result of the previous customer to STDOUT\n",
    "            print(\"{}\\t{}\".format(currentCustomer, currentConsumption))\n",
    "        # we set a new currentCustomer and a new currentConsumption\n",
    "        currentConsumption = consumptionPerDay\n",
    "        currentCustomer = customerID_date\n",
    "\n",
    "#printing the very last customer also to STDOUT\n",
    "if currentCustomer == customerID_date:\n",
    "    print(\"{}\\t{}\".format(currentCustomer, currentConsumption))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ad412d-fc38-4ed5-a4da-8c0b33dd4682",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run job\n",
    "!mapred streaming -files mapper.py,reducer.py -mapper mapper.py -reducer reducer.py -input /data/consumer_data.csv -output output\n",
    "!hdfs dfs -cat output/part-00000 | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76737837-416d-431b-ab51-2d974a20064e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1018508_2016-10-19\t12\n",
      "1018508_2016-10-20\t14\n",
      "1018508_2016-10-21\t14\n",
      "1018508_2016-10-23\t4\n",
      "1018508_2016-10-25\t16\n",
      "1018508_2016-10-30\t4\n",
      "1018508_2016-11-03\t11\n",
      "1018508_2016-11-05\t36\n",
      "1018508_2016-11-06\t4\n",
      "1018508_2016-11-08\t25\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat output16/part-00000 | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d0750e-dd58-41bc-9e40-017d76085427",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
